2025-02-11 17:05:41,593:[P:4213]:Rank[0/1] => collecting env info (might take some time)
2025-02-11 17:06:01,428:[P:4213]:Rank[0/1] 
PyTorch version: 1.7.1+cu110
Is debug build: False
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.9 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070
Nvidia driver version: 560.94
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] torch==1.7.1+cu110
[pip3] torchinfo==1.8.0
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.8.2+cu110
[conda] blas                      1.0                         mkl  
[conda] cpuonly                   2.0                           0    pytorch
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py39he8ac12f_0  
[conda] mkl_fft                   1.3.0            py39h54f3939_0  
[conda] mkl_random                1.0.2            py39h63df603_0  
[conda] numpy                     1.25.2                   pypi_0    pypi
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] torch                     1.7.1+cu110              pypi_0    pypi
[conda] torchinfo                 1.8.0                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.8.2+cu110              pypi_0    pypi
2025-02-11 17:06:01,428:[P:4213]:Rank[0/1] Namespace(cfg='experiments/imagenet/cvt/cvt-dim6-310x321.yaml', local_rank=0, port=9000, opts=[], num_gpus=1, distributed=False)
2025-02-11 17:06:01,428:[P:4213]:Rank[0/1] AMP:
  ENABLED: True
  MEMORY_FORMAT: nchw
AUG:
  COLOR_JITTER: [0.4, 0.4, 0.4, 0.1, 0.0]
  DROPBLOCK_BLOCK_SIZE: 7
  DROPBLOCK_KEEP_PROB: 1.0
  DROPBLOCK_LAYERS: [3, 4]
  GAUSSIAN_BLUR: 0.0
  GRAY_SCALE: 0.0
  INTERPOLATION: 2
  MIXCUT: 1.0
  MIXCUT_AND_MIXUP: False
  MIXCUT_MINMAX: []
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RATIO: (0.75, 1.3333333333333333)
  SCALE: (0.08, 1.0)
  TIMM_AUG:
    AUTO_AUGMENT: rand-m9-mstd0.5-inc1
    COLOR_JITTER: 0.4
    HFLIP: 0.5
    INTERPOLATION: bicubic
    RE_COUNT: 1
    RE_MODE: pixel
    RE_PROB: 0.25
    RE_SPLIT: False
    USE_LOADER: False
    USE_TRANSFORM: False
    VFLIP: 0.0
BASE: ['']
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: multidim_imagenet
  DATA_FORMAT: jpg
  LABELMAP: 
  ROOT: DATASET/agmel/
  SAMPLER: default
  TARGET_SIZE: -1
  TEST_SET: val
  TEST_TSV_LIST: []
  TRAIN_SET: train
  TRAIN_TSV_LIST: []
DATA_DIR: 
DEBUG:
  DEBUG: False
DIST_BACKEND: nccl
FINETUNE:
  BASE_LR: 0.003
  BATCH_SIZE: 512
  EVAL_EVERY: 3000
  FINETUNE: False
  FROZEN_LAYERS: []
  LR_SCHEDULER:
    DECAY_TYPE: step
  TRAIN_MODE: True
  USE_TRAIN_AUG: False
GPUS: (0,)
INPUT:
  MEAN: [0.485, 0.456, 0.406]
  STD: [0.229, 0.224, 0.225]
LOSS:
  LABEL_SMOOTHING: 0.1
  LOSS: softmax
MODEL:
  INIT_WEIGHTS: True
  NAME: cls_cvt
  NUM_CLASSES: 1000
  PRETRAINED: 
  PRETRAINED_LAYERS: ['*']
  SPEC:
    ATTN_DROP_RATE: [0.0, 0.0, 0.0]
    CLS_TOKEN: [False, False, True]
    DEPTH: [1, 2, 10]
    DIM_EMBED: [64, 192, 384]
    DROP_PATH_RATE: [0.0, 0.0, 0.1]
    DROP_RATE: [0.0, 0.0, 0.0]
    INIT: trunc_norm
    KERNEL_QKV: [3, 3, 3]
    MLP_RATIO: [4.0, 4.0, 4.0]
    NUM_HEADS: [1, 3, 6]
    NUM_STAGES: 3
    PADDING_KV: [1, 1, 1]
    PADDING_Q: [1, 1, 1]
    PATCH_PADDING: [2, 1, 1]
    PATCH_SIZE: [7, 3, 3]
    PATCH_STRIDE: [4, 2, 2]
    POS_EMBED: [False, False, False]
    QKV_BIAS: [True, True, True]
    QKV_PROJ_METHOD: ['dw_bn', 'dw_bn', 'dw_bn']
    STRIDE_KV: [2, 2, 2]
    STRIDE_Q: [1, 1, 1]
MODEL_SUMMARY: False
MULTIPROCESSING_DISTRIBUTED: True
NAME: cvt-dim6-310x321
OUTPUT_DIR: OUTPUT/
PIN_MEMORY: True
PRINT_FREQ: 500
RANK: 0
TEST:
  BATCH_SIZE_PER_GPU: 8
  CENTER_CROP: True
  IMAGE_SIZE: [310, 321]
  INTERPOLATION: 3
  MODEL_FILE: 
  REAL_LABELS: False
  VALID_LABELS: 
TRAIN:
  AUTO_RESUME: True
  BATCH_SIZE_PER_GPU: 8
  BEGIN_EPOCH: 0
  CHECKPOINT: 
  CLIP_GRAD_NORM: 0.0
  DETECT_ANOMALY: False
  END_EPOCH: 10
  EVAL_BEGIN_EPOCH: 0
  GAMMA1: 0.99
  GAMMA2: 0.0
  IMAGE_SIZE: [310, 321]
  LR: 0.00025
  LR_SCHEDULER:
    ARGS:
      cooldown_epochs: 10
      decay_rate: 0.1
      epochs: 10
      min_lr: 1e-05
      sched: cosine
      warmup_epochs: 5
      warmup_lr: 1e-06
    METHOD: timm
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZER: adamW
  OPTIMIZER_ARGS:
    
  SAVE_ALL_MODELS: False
  SCALE_LR: True
  SHUFFLE: True
  WD: 0.05
  WITHOUT_WD_LIST: ['bn', 'bias', 'ln']
VERBOSE: True
WORKERS: 4
2025-02-11 17:06:01,430:[P:4213]:Rank[0/1] => using 1 GPUs
2025-02-11 17:06:01,430:[P:4213]:Rank[0/1] => saving config into: OUTPUT/multidim_imagenet/cvt-dim6-310x321/config.yaml
2025-02-11 17:06:01,468:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,468:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,468:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,469:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,469:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,469:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,469:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,469:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,470:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,470:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,470:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,480:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,622:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,623:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,623:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,633:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,633:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,634:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,634:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,634:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,635:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,636:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,636:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,639:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,649:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,650:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,650:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,650:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,651:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,651:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,651:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,652:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,652:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,653:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:01,653:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:01,665:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,123:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,125:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,125:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,136:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,136:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,137:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,138:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,141:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,142:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,159:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,159:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,175:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,175:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,177:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,177:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,178:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,179:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,191:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,191:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,193:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,193:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,208:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,208:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,224:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,224:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,225:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,226:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,227:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,227:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,238:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,238:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,240:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,240:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,255:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,255:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,261:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,270:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,272:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,272:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,274:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,274:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,275:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,275:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,286:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,287:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,292:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,300:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,307:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,307:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,308:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,309:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,310:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,311:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,312:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,313:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,324:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,325:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,330:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,331:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,346:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,347:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,348:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,348:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,350:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,360:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,362:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,362:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,364:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,364:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,370:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,371:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,387:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,387:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,389:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,389:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,391:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,400:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,402:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,402:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,403:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,404:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,413:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,413:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,420:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,420:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,425:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,425:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,427:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,427:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,443:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,444:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,445:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,445:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,453:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,453:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,468:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,469:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,471:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,471:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,472:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,472:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,486:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,486:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,487:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,487:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,492:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,493:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,509:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,509:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,521:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,521:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,537:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,537:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,542:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,542:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,551:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,551:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,574:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:02,574:[P:4213]:Rank[0/1] => init weight of Linear from trunc norm
2025-02-11 17:06:02,593:[P:4213]:Rank[0/1] => init bias of Linear to zeros
2025-02-11 17:06:39,715:[P:4213]:Rank[0/1] => ConvolutionalVisionTransformer(
  (stage0): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=64, out_features=64, bias=True)
          (proj_k): Linear(in_features=64, out_features=64, bias=True)
          (proj_v): Linear(in_features=64, out_features=64, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (stage1): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=192, out_features=192, bias=True)
          (proj_k): Linear(in_features=192, out_features=192, bias=True)
          (proj_v): Linear(in_features=192, out_features=192, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=192, out_features=192, bias=True)
          (proj_k): Linear(in_features=192, out_features=192, bias=True)
          (proj_v): Linear(in_features=192, out_features=192, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (stage2): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=384, out_features=1000, bias=True)
  (merge_attn): MergeAttention(
    (mha): MultiheadAttention(
      (out_proj): _LinearWithBias(in_features=384, out_features=384, bias=True)
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
)
2025-02-11 17:06:39,730:[P:4213]:Rank[0/1] Trainable Model Total Parameter: 	20.6M
2025-02-11 17:06:39,802:[P:4213]:Rank[0/1] => MultiDimImageDataset from 'DATASET/agmel/train' loaded. total samples: 368
2025-02-11 17:06:39,802:[P:4213]:Rank[0/1] => [build_dataset] dataset=multidim_imagenet, is_train=True, #samples=368
2025-02-11 17:06:41,110:[P:4213]:Rank[0/1] [DEBUG] Batch shape: torch.Size([8, 6, 3, 310, 310])
2025-02-11 17:06:41,531:[P:4213]:Rank[0/1] => MultiDimImageDataset from 'DATASET/agmel/val' loaded. total samples: 94
2025-02-11 17:06:41,532:[P:4213]:Rank[0/1] => [build_dataset] dataset=multidim_imagenet, is_train=False, #samples=94
2025-02-11 17:06:42,529:[P:4213]:Rank[0/1] [DEBUG] Batch shape: torch.Size([8, 6, 3, 310, 310])
2025-02-11 17:06:43,242:[P:4213]:Rank[0/1] => start training
2025-02-11 17:06:43,243:[P:4213]:Rank[0/1] => Epoch[0]: epoch start
2025-02-11 17:06:43,243:[P:4213]:Rank[0/1] => Epoch[0]: train start
2025-02-11 17:06:43,243:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:07:25,479:[P:4213]:Rank[0/1] => Epoch[0][0/46]: Time 42.234s (42.234s)	Speed 0.2 samples/s	Data 1.046s (1.046s)	Loss 6.75762 (6.75762)	Accuracy@1 0.000 (0.000)	Accuracy@5 0.000 (0.000)	
2025-02-11 17:20:42,033:[P:4213]:Rank[0/1] => Epoch[0]: train end, duration: 838.79s
2025-02-11 17:20:42,035:[P:4213]:Rank[0/1] => Epoch[0]: validate start
2025-02-11 17:20:42,035:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 17:20:57,543:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 17:20:57,543:[P:4213]:Rank[0/1] => TEST:	Loss 4.4656	Error@1 5.319%	Error@5 5.319%	Accuracy@1 94.681%	Accuracy@5 94.681%	
2025-02-11 17:20:57,544:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:20:57,546:[P:4213]:Rank[0/1] => Epoch[0]: validate end, duration: 15.51s
2025-02-11 17:20:57,547:[P:4213]:Rank[0/1] => lr: 5.0799999999999995e-05
2025-02-11 17:20:57,558:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 17:20:58,251:[P:4213]:Rank[0/1] => save model to OUTPUT/multidim_imagenet/cvt-dim6-310x321/model_best.pth
2025-02-11 17:20:58,562:[P:4213]:Rank[0/1] => Epoch[0]: epoch end, duration : 855.32s
2025-02-11 17:20:58,563:[P:4213]:Rank[0/1] => Epoch[1]: epoch start
2025-02-11 17:20:58,563:[P:4213]:Rank[0/1] => Epoch[1]: train start
2025-02-11 17:20:58,563:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:21:20,865:[P:4213]:Rank[0/1] => Epoch[1][0/46]: Time 22.300s (22.300s)	Speed 0.4 samples/s	Data 0.882s (0.882s)	Loss 4.65958 (4.65958)	Accuracy@1 100.000 (100.000)	Accuracy@5 100.000 (100.000)	
2025-02-11 17:34:43,659:[P:4213]:Rank[0/1] => Epoch[1]: train end, duration: 825.10s
2025-02-11 17:34:43,673:[P:4213]:Rank[0/1] => Epoch[1]: validate start
2025-02-11 17:34:43,674:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 17:36:27,859:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 17:36:27,859:[P:4213]:Rank[0/1] => TEST:	Loss 0.4902	Error@1 5.319%	Error@5 0.000%	Accuracy@1 94.681%	Accuracy@5 100.000%	
2025-02-11 17:36:27,860:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:36:27,862:[P:4213]:Rank[0/1] => Epoch[1]: validate end, duration: 104.19s
2025-02-11 17:36:27,862:[P:4213]:Rank[0/1] => lr: 0.00010059999999999999
2025-02-11 17:36:27,865:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 17:36:30,753:[P:4213]:Rank[0/1] => Epoch[1]: epoch end, duration : 932.19s
2025-02-11 17:36:30,753:[P:4213]:Rank[0/1] => Epoch[2]: epoch start
2025-02-11 17:36:30,753:[P:4213]:Rank[0/1] => Epoch[2]: train start
2025-02-11 17:36:30,754:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:36:48,823:[P:4213]:Rank[0/1] => Epoch[2][0/46]: Time 18.068s (18.068s)	Speed 0.4 samples/s	Data 0.801s (0.801s)	Loss 1.61185 (1.61185)	Accuracy@1 87.500 (87.500)	Accuracy@5 100.000 (100.000)	
2025-02-11 17:50:08,407:[P:4213]:Rank[0/1] => Epoch[2]: train end, duration: 817.65s
2025-02-11 17:50:08,420:[P:4213]:Rank[0/1] => Epoch[2]: validate start
2025-02-11 17:50:08,421:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 17:51:50,126:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 17:51:50,127:[P:4213]:Rank[0/1] => TEST:	Loss 0.1613	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 17:51:50,128:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:51:50,130:[P:4213]:Rank[0/1] => Epoch[2]: validate end, duration: 101.71s
2025-02-11 17:51:50,131:[P:4213]:Rank[0/1] => lr: 0.0001504
2025-02-11 17:51:50,134:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 17:51:52,981:[P:4213]:Rank[0/1] => save model to OUTPUT/multidim_imagenet/cvt-dim6-310x321/model_best.pth
2025-02-11 17:51:53,271:[P:4213]:Rank[0/1] => Epoch[2]: epoch end, duration : 922.52s
2025-02-11 17:51:53,271:[P:4213]:Rank[0/1] => Epoch[3]: epoch start
2025-02-11 17:51:53,271:[P:4213]:Rank[0/1] => Epoch[3]: train start
2025-02-11 17:51:53,271:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 17:52:11,169:[P:4213]:Rank[0/1] => Epoch[3][0/46]: Time 17.896s (17.896s)	Speed 0.4 samples/s	Data 0.824s (0.824s)	Loss 1.33094 (1.33094)	Accuracy@1 87.500 (87.500)	Accuracy@5 100.000 (100.000)	
2025-02-11 18:05:40,331:[P:4213]:Rank[0/1] => Epoch[3]: train end, duration: 827.06s
2025-02-11 18:05:40,335:[P:4213]:Rank[0/1] => Epoch[3]: validate start
2025-02-11 18:05:40,335:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 18:07:24,160:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 18:07:24,161:[P:4213]:Rank[0/1] => TEST:	Loss 0.1294	Error@1 3.191%	Error@5 0.000%	Accuracy@1 96.809%	Accuracy@5 100.000%	
2025-02-11 18:07:24,162:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:07:24,164:[P:4213]:Rank[0/1] => Epoch[3]: validate end, duration: 103.83s
2025-02-11 18:07:24,165:[P:4213]:Rank[0/1] => lr: 0.0002002
2025-02-11 18:07:24,167:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 18:07:27,011:[P:4213]:Rank[0/1] => Epoch[3]: epoch end, duration : 933.74s
2025-02-11 18:07:27,011:[P:4213]:Rank[0/1] => Epoch[4]: epoch start
2025-02-11 18:07:27,012:[P:4213]:Rank[0/1] => Epoch[4]: train start
2025-02-11 18:07:27,012:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:07:48,506:[P:4213]:Rank[0/1] => Epoch[4][0/46]: Time 21.493s (21.493s)	Speed 0.4 samples/s	Data 0.767s (0.767s)	Loss 1.62995 (1.62995)	Accuracy@1 87.500 (87.500)	Accuracy@5 100.000 (100.000)	
2025-02-11 18:21:06,714:[P:4213]:Rank[0/1] => Epoch[4]: train end, duration: 819.70s
2025-02-11 18:21:06,715:[P:4213]:Rank[0/1] => Epoch[4]: validate start
2025-02-11 18:21:06,716:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 18:22:47,160:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 18:22:47,162:[P:4213]:Rank[0/1] => TEST:	Loss 0.1396	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 18:22:47,162:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:22:47,164:[P:4213]:Rank[0/1] => Epoch[4]: validate end, duration: 100.45s
2025-02-11 18:22:47,165:[P:4213]:Rank[0/1] => lr: 0.00013000000000000002
2025-02-11 18:22:47,168:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 18:22:50,132:[P:4213]:Rank[0/1] => Epoch[4]: epoch end, duration : 923.12s
2025-02-11 18:22:50,133:[P:4213]:Rank[0/1] => Epoch[5]: epoch start
2025-02-11 18:22:50,133:[P:4213]:Rank[0/1] => Epoch[5]: train start
2025-02-11 18:22:50,133:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:23:08,237:[P:4213]:Rank[0/1] => Epoch[5][0/46]: Time 18.102s (18.102s)	Speed 0.4 samples/s	Data 0.888s (0.888s)	Loss 1.04934 (1.04934)	Accuracy@1 100.000 (100.000)	Accuracy@5 100.000 (100.000)	
2025-02-11 18:36:23,136:[P:4213]:Rank[0/1] => Epoch[5]: train end, duration: 813.00s
2025-02-11 18:36:23,140:[P:4213]:Rank[0/1] => Epoch[5]: validate start
2025-02-11 18:36:23,141:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 18:38:10,639:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 18:38:10,640:[P:4213]:Rank[0/1] => TEST:	Loss 0.1123	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 18:38:10,640:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:38:10,643:[P:4213]:Rank[0/1] => Epoch[5]: validate end, duration: 107.50s
2025-02-11 18:38:10,644:[P:4213]:Rank[0/1] => lr: 9.291796067500632e-05
2025-02-11 18:38:10,657:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 18:38:13,654:[P:4213]:Rank[0/1] => Epoch[5]: epoch end, duration : 923.52s
2025-02-11 18:38:13,665:[P:4213]:Rank[0/1] => Epoch[6]: epoch start
2025-02-11 18:38:13,665:[P:4213]:Rank[0/1] => Epoch[6]: train start
2025-02-11 18:38:13,665:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:38:31,799:[P:4213]:Rank[0/1] => Epoch[6][0/46]: Time 18.132s (18.132s)	Speed 0.4 samples/s	Data 1.014s (1.014s)	Loss 1.02818 (1.02818)	Accuracy@1 100.000 (100.000)	Accuracy@5 100.000 (100.000)	
2025-02-11 18:52:06,356:[P:4213]:Rank[0/1] => Epoch[6]: train end, duration: 832.69s
2025-02-11 18:52:06,358:[P:4213]:Rank[0/1] => Epoch[6]: validate start
2025-02-11 18:52:06,358:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 18:53:44,562:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 18:53:44,563:[P:4213]:Rank[0/1] => TEST:	Loss 0.0991	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 18:53:44,564:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:53:44,566:[P:4213]:Rank[0/1] => Epoch[6]: validate end, duration: 98.21s
2025-02-11 18:53:44,566:[P:4213]:Rank[0/1] => lr: 5.9465769724903235e-05
2025-02-11 18:53:44,569:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 18:53:47,422:[P:4213]:Rank[0/1] => Epoch[6]: epoch end, duration : 933.76s
2025-02-11 18:53:47,422:[P:4213]:Rank[0/1] => Epoch[7]: epoch start
2025-02-11 18:53:47,422:[P:4213]:Rank[0/1] => Epoch[7]: train start
2025-02-11 18:53:47,422:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 18:54:05,961:[P:4213]:Rank[0/1] => Epoch[7][0/46]: Time 18.537s (18.537s)	Speed 0.4 samples/s	Data 0.770s (0.770s)	Loss 1.36019 (1.36019)	Accuracy@1 87.500 (87.500)	Accuracy@5 100.000 (100.000)	
2025-02-11 19:07:25,910:[P:4213]:Rank[0/1] => Epoch[7]: train end, duration: 818.49s
2025-02-11 19:07:25,924:[P:4213]:Rank[0/1] => Epoch[7]: validate start
2025-02-11 19:07:25,925:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 19:09:12,409:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 19:09:12,410:[P:4213]:Rank[0/1] => TEST:	Loss 0.1080	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 19:09:12,411:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 19:09:12,413:[P:4213]:Rank[0/1] => Epoch[7]: validate end, duration: 106.49s
2025-02-11 19:09:12,414:[P:4213]:Rank[0/1] => lr: 3.291796067500632e-05
2025-02-11 19:09:12,416:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 19:09:15,306:[P:4213]:Rank[0/1] => Epoch[7]: epoch end, duration : 927.88s
2025-02-11 19:09:15,306:[P:4213]:Rank[0/1] => Epoch[8]: epoch start
2025-02-11 19:09:15,306:[P:4213]:Rank[0/1] => Epoch[8]: train start
2025-02-11 19:09:15,306:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 19:09:33,878:[P:4213]:Rank[0/1] => Epoch[8][0/46]: Time 18.559s (18.559s)	Speed 0.4 samples/s	Data 0.627s (0.627s)	Loss 1.15727 (1.15727)	Accuracy@1 100.000 (100.000)	Accuracy@5 100.000 (100.000)	
2025-02-11 19:22:56,963:[P:4213]:Rank[0/1] => Epoch[8]: train end, duration: 821.66s
2025-02-11 19:22:56,978:[P:4213]:Rank[0/1] => Epoch[8]: validate start
2025-02-11 19:22:56,979:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 19:24:39,171:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 19:24:39,173:[P:4213]:Rank[0/1] => TEST:	Loss 0.1143	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 19:24:39,173:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 19:24:39,175:[P:4213]:Rank[0/1] => Epoch[8]: validate end, duration: 102.20s
2025-02-11 19:24:39,176:[P:4213]:Rank[0/1] => lr: 1.587321804458158e-05
2025-02-11 19:24:39,179:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 19:24:42,256:[P:4213]:Rank[0/1] => Epoch[8]: epoch end, duration : 926.95s
2025-02-11 19:24:42,257:[P:4213]:Rank[0/1] => Epoch[9]: epoch start
2025-02-11 19:24:42,257:[P:4213]:Rank[0/1] => Epoch[9]: train start
2025-02-11 19:24:42,257:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 19:25:00,084:[P:4213]:Rank[0/1] => Epoch[9][0/46]: Time 17.826s (17.826s)	Speed 0.4 samples/s	Data 0.837s (0.837s)	Loss 1.03183 (1.03183)	Accuracy@1 100.000 (100.000)	Accuracy@5 100.000 (100.000)	
2025-02-11 19:38:33,734:[P:4213]:Rank[0/1] => Epoch[9]: train end, duration: 831.48s
2025-02-11 19:38:33,747:[P:4213]:Rank[0/1] => Epoch[9]: validate start
2025-02-11 19:38:33,748:[P:4213]:Rank[0/1] => switch to eval mode
2025-02-11 19:40:14,345:[P:4213]:Rank[0/1] => synchronize...
2025-02-11 19:40:14,347:[P:4213]:Rank[0/1] => TEST:	Loss 0.1170	Error@1 0.000%	Error@5 0.000%	Accuracy@1 100.000%	Accuracy@5 100.000%	
2025-02-11 19:40:14,347:[P:4213]:Rank[0/1] => switch to train mode
2025-02-11 19:40:14,349:[P:4213]:Rank[0/1] => Epoch[9]: validate end, duration: 100.60s
2025-02-11 19:40:14,350:[P:4213]:Rank[0/1] => lr: 1e-05
2025-02-11 19:40:14,353:[P:4213]:Rank[0/1] => saving checkpoint to OUTPUT/multidim_imagenet/cvt-dim6-310x321
2025-02-11 19:40:17,290:[P:4213]:Rank[0/1] => Epoch[9]: epoch end, duration : 935.03s
2025-02-11 19:40:17,290:[P:4213]:Rank[0/1] => save model to OUTPUT/multidim_imagenet/cvt-dim6-310x321/final_state.pth
2025-02-11 19:40:17,505:[P:4213]:Rank[0/1] => finish training
