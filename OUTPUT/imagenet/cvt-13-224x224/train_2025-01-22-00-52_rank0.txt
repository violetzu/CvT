2025-01-22 00:52:52,122:[P:2526]:Rank[0/1] => collecting env info (might take some time)
2025-01-22 00:52:56,708:[P:2526]:Rank[0/1] 
PyTorch version: 1.7.1+cu110
Is debug build: False
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.9 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070
Nvidia driver version: 536.23
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.3
[pip3] torch==1.7.1+cu110
[pip3] torchvision==0.8.2+cu110
[conda] _pytorch_select           0.1                       cpu_0  
[conda] blas                      1.0                         mkl  
[conda] cpuonly                   2.0                           0    pytorch
[conda] libmklml                  2019.0.5             h06a4308_0  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py39he8ac12f_0  
[conda] mkl_fft                   1.3.0            py39h54f3939_0  
[conda] mkl_random                1.0.2            py39h63df603_0  
[conda] numpy                     1.19.3                   pypi_0    pypi
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] torch                     1.7.1+cu110              pypi_0    pypi
[conda] torchvision               0.8.2+cu110              pypi_0    pypi
2025-01-22 00:52:56,708:[P:2526]:Rank[0/1] Namespace(cfg='experiments/imagenet/cvt/cvt-13-224x224.yaml', local_rank=0, port=9000, opts=[], num_gpus=1, distributed=False)
2025-01-22 00:52:56,709:[P:2526]:Rank[0/1] AMP:
  ENABLED: True
  MEMORY_FORMAT: nchw
AUG:
  COLOR_JITTER: [0.4, 0.4, 0.4, 0.1, 0.0]
  DROPBLOCK_BLOCK_SIZE: 7
  DROPBLOCK_KEEP_PROB: 1.0
  DROPBLOCK_LAYERS: [3, 4]
  GAUSSIAN_BLUR: 0.0
  GRAY_SCALE: 0.0
  INTERPOLATION: 2
  MIXCUT: 1.0
  MIXCUT_AND_MIXUP: False
  MIXCUT_MINMAX: []
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RATIO: (0.75, 1.3333333333333333)
  SCALE: (0.08, 1.0)
  TIMM_AUG:
    AUTO_AUGMENT: rand-m9-mstd0.5-inc1
    COLOR_JITTER: 0.4
    HFLIP: 0.5
    INTERPOLATION: bicubic
    RE_COUNT: 1
    RE_MODE: pixel
    RE_PROB: 0.25
    RE_SPLIT: False
    USE_LOADER: True
    USE_TRANSFORM: False
    VFLIP: 0.0
BASE: ['']
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: imagenet
  DATA_FORMAT: jpg
  LABELMAP: 
  ROOT: DATASET/imagenet/
  SAMPLER: default
  TARGET_SIZE: -1
  TEST_SET: val
  TEST_TSV_LIST: []
  TRAIN_SET: train
  TRAIN_TSV_LIST: []
DATA_DIR: 
DEBUG:
  DEBUG: False
DIST_BACKEND: nccl
FINETUNE:
  BASE_LR: 0.003
  BATCH_SIZE: 512
  EVAL_EVERY: 3000
  FINETUNE: False
  FROZEN_LAYERS: []
  LR_SCHEDULER:
    DECAY_TYPE: step
  TRAIN_MODE: True
  USE_TRAIN_AUG: False
GPUS: (0,)
INPUT:
  MEAN: [0.485, 0.456, 0.406]
  STD: [0.229, 0.224, 0.225]
LOSS:
  LABEL_SMOOTHING: 0.1
  LOSS: softmax
MODEL:
  INIT_WEIGHTS: True
  NAME: cls_cvt
  NUM_CLASSES: 1000
  PRETRAINED: 
  PRETRAINED_LAYERS: ['*']
  SPEC:
    ATTN_DROP_RATE: [0.0, 0.0, 0.0]
    CLS_TOKEN: [False, False, True]
    DEPTH: [1, 2, 10]
    DIM_EMBED: [64, 192, 384]
    DROP_PATH_RATE: [0.0, 0.0, 0.1]
    DROP_RATE: [0.0, 0.0, 0.0]
    INIT: trunc_norm
    KERNEL_QKV: [3, 3, 3]
    MLP_RATIO: [4.0, 4.0, 4.0]
    NUM_HEADS: [1, 3, 6]
    NUM_STAGES: 3
    PADDING_KV: [1, 1, 1]
    PADDING_Q: [1, 1, 1]
    PATCH_PADDING: [2, 1, 1]
    PATCH_SIZE: [7, 3, 3]
    PATCH_STRIDE: [4, 2, 2]
    POS_EMBED: [False, False, False]
    QKV_BIAS: [True, True, True]
    QKV_PROJ_METHOD: ['dw_bn', 'dw_bn', 'dw_bn']
    STRIDE_KV: [2, 2, 2]
    STRIDE_Q: [1, 1, 1]
MODEL_SUMMARY: False
MULTIPROCESSING_DISTRIBUTED: True
NAME: cvt-13-224x224
OUTPUT_DIR: OUTPUT/
PIN_MEMORY: True
PRINT_FREQ: 500
RANK: 0
TEST:
  BATCH_SIZE_PER_GPU: 32
  CENTER_CROP: True
  IMAGE_SIZE: [224, 224]
  INTERPOLATION: 3
  MODEL_FILE: 
  REAL_LABELS: False
  VALID_LABELS: 
TRAIN:
  AUTO_RESUME: True
  BATCH_SIZE_PER_GPU: 64
  BEGIN_EPOCH: 0
  CHECKPOINT: 
  CLIP_GRAD_NORM: 0.0
  DETECT_ANOMALY: False
  END_EPOCH: 2
  EVAL_BEGIN_EPOCH: 0
  GAMMA1: 0.99
  GAMMA2: 0.0
  IMAGE_SIZE: [224, 224]
  LR: 0.00025
  LR_SCHEDULER:
    ARGS:
      cooldown_epochs: 10
      decay_rate: 0.1
      epochs: 2
      min_lr: 1e-05
      sched: cosine
      warmup_epochs: 5
      warmup_lr: 1e-06
    METHOD: timm
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZER: adamW
  OPTIMIZER_ARGS:
    
  SAVE_ALL_MODELS: False
  SCALE_LR: True
  SHUFFLE: True
  WD: 0.05
  WITHOUT_WD_LIST: ['bn', 'bias', 'ln']
VERBOSE: True
WORKERS: 6
2025-01-22 00:52:56,709:[P:2526]:Rank[0/1] => using 1 GPUs
2025-01-22 00:52:56,709:[P:2526]:Rank[0/1] => saving config into: OUTPUT/imagenet/cvt-13-224x224/config.yaml
2025-01-22 00:52:56,720:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,721:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,722:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,722:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,722:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,722:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,732:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,733:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,733:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,733:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,733:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,734:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,734:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,734:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,734:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,735:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,735:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,736:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,736:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,737:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,737:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,737:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,737:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,738:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,738:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,738:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,738:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,739:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,739:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,741:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,852:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,853:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,853:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,854:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,854:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,855:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,855:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,856:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,856:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,859:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,859:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,863:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,864:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,865:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,865:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,866:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,866:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,867:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,867:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,868:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,868:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,871:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,871:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,874:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,875:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,876:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,876:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,878:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,878:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,879:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,879:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,880:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,881:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,884:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,884:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,887:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,888:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,889:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,889:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,890:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,891:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,891:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,892:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,892:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,892:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,895:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,895:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,898:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,899:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,900:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,900:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,902:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,902:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,903:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,904:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,904:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,905:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,908:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,908:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,911:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,911:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,912:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,912:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,914:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,914:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,915:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,915:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,917:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,917:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,920:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,920:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,923:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,923:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,924:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,924:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,925:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,925:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,926:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,927:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,928:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,928:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,932:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,932:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,935:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,935:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,936:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,936:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,937:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,937:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,938:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,938:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,939:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,939:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,944:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,944:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,947:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,947:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,948:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,948:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,949:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,949:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,950:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,950:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,951:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,951:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,955:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,955:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,959:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,959:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,960:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,960:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,961:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,961:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,962:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,962:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,963:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,963:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,966:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:52:56,966:[P:2526]:Rank[0/1] => init weight of Linear from trunc norm
2025-01-22 00:52:56,970:[P:2526]:Rank[0/1] => init bias of Linear to zeros
2025-01-22 00:53:24,074:[P:2526]:Rank[0/1] => ConvolutionalVisionTransformer(
  (stage0): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=64, out_features=64, bias=True)
          (proj_k): Linear(in_features=64, out_features=64, bias=True)
          (proj_v): Linear(in_features=64, out_features=64, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (stage1): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=192, out_features=192, bias=True)
          (proj_k): Linear(in_features=192, out_features=192, bias=True)
          (proj_v): Linear(in_features=192, out_features=192, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=192, out_features=192, bias=True)
          (proj_k): Linear(in_features=192, out_features=192, bias=True)
          (proj_v): Linear(in_features=192, out_features=192, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (stage2): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=384, out_features=1000, bias=True)
)
2025-01-22 00:53:24,076:[P:2526]:Rank[0/1] Trainable Model Total Parameter: 	20.0M
2025-01-22 00:53:24,088:[P:2526]:Rank[0/1] => use timm loader for training
2025-01-22 00:53:24,199:[P:2526]:Rank[0/1] => start training
2025-01-22 00:53:24,199:[P:2526]:Rank[0/1] => Epoch[0]: epoch start
2025-01-22 00:53:24,199:[P:2526]:Rank[0/1] => Epoch[0]: train start
2025-01-22 00:53:24,199:[P:2526]:Rank[0/1] => switch to train mode
2025-01-22 00:53:24,919:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:53:24,919:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:53:24,919:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:53:25,176:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:53:25,176:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:53:25,176:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:53:26,016:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:53:26,016:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:53:26,016:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:53:47,367:[P:2526]:Rank[0/1] => Epoch[0][0/13]: Time 23.166s (23.166s)	Speed 2.8 samples/s	Data 0.423s (0.423s)	Loss 7.00479 (7.00479)	Accuracy@1 0.000 (0.000)	Accuracy@5 0.000 (0.000)	
2025-01-22 00:53:47,389:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:53:47,390:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:53:47,390:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:53:47,417:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:53:47,417:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:53:47,417:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:53:47,657:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:53:47,657:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:53:47,657:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:53:53,864:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:53:53,865:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:53:53,865:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:53:53,869:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:53:53,869:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:53:53,869:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:53:53,897:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:53:53,897:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:53:53,897:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:00,839:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:00,840:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:00,840:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:00,844:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:00,844:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:00,844:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:00,871:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:00,871:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:00,871:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:07,641:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:07,641:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:07,642:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:07,646:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:07,646:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:07,646:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:07,676:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:07,676:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:07,676:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:14,313:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:14,313:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:14,313:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:14,318:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:14,318:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:14,318:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:14,344:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:14,345:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:14,345:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:20,957:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:20,957:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:20,957:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:20,961:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:20,961:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:20,962:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:20,987:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:20,987:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:20,987:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:27,594:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:27,595:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:27,595:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:27,599:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:27,599:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:27,599:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:27,626:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:27,626:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:27,626:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:34,245:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:34,245:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:34,245:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:34,250:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:34,250:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:34,250:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:34,277:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:34,278:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:34,278:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:40,804:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:40,804:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:40,804:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:40,808:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:40,808:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:40,809:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:40,838:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:40,838:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:40,838:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:47,398:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:47,399:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:47,399:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:47,403:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:47,403:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:47,403:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:47,428:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:47,428:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:47,428:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:54:54,539:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:54:54,539:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:54,539:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:54,544:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:54:54,544:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:54,544:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:54:54,570:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:54:54,570:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:54:54,570:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:00,938:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:00,939:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:00,939:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:00,943:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:00,943:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:00,943:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:00,968:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:00,968:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:00,969:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:07,805:[P:2526]:Rank[0/1] => Epoch[0]: train end, duration: 103.61s
2025-01-22 00:55:07,805:[P:2526]:Rank[0/1] => Epoch[0]: validate start
2025-01-22 00:55:07,805:[P:2526]:Rank[0/1] => switch to eval mode
2025-01-22 00:55:08,929:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 3, 224, 224])
2025-01-22 00:55:08,929:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:55:08,929:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:09,169:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:55:09,170:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:55:09,170:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:09,451:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:55:09,451:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 384, 14, 14])
2025-01-22 00:55:09,451:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([32, 1, 384])
2025-01-22 00:55:10,168:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 3, 224, 224])
2025-01-22 00:55:10,169:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:55:10,169:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:10,172:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:55:10,172:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:55:10,172:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:10,192:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:55:10,192:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 384, 14, 14])
2025-01-22 00:55:10,192:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([32, 1, 384])
2025-01-22 00:55:11,287:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([29, 3, 224, 224])
2025-01-22 00:55:11,288:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([29, 64, 56, 56])
2025-01-22 00:55:11,288:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:11,777:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([29, 64, 56, 56])
2025-01-22 00:55:11,777:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([29, 192, 28, 28])
2025-01-22 00:55:11,777:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:12,015:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([29, 192, 28, 28])
2025-01-22 00:55:12,015:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([29, 384, 14, 14])
2025-01-22 00:55:12,015:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([29, 1, 384])
2025-01-22 00:55:12,829:[P:2526]:Rank[0/1] => synchronize...
2025-01-22 00:55:12,829:[P:2526]:Rank[0/1] => TEST:	Loss 6.7628	Error@1 100.000%	Error@5 96.774%	Accuracy@1 0.000%	Accuracy@5 3.226%	
2025-01-22 00:55:12,829:[P:2526]:Rank[0/1] => switch to train mode
2025-01-22 00:55:12,831:[P:2526]:Rank[0/1] => Epoch[0]: validate end, duration: 5.03s
2025-01-22 00:55:12,831:[P:2526]:Rank[0/1] => lr: 5.0799999999999995e-05
2025-01-22 00:55:12,833:[P:2526]:Rank[0/1] => saving checkpoint to OUTPUT/imagenet/cvt-13-224x224
2025-01-22 00:55:13,199:[P:2526]:Rank[0/1] => Epoch[0]: epoch end, duration : 109.00s
2025-01-22 00:55:13,199:[P:2526]:Rank[0/1] => Epoch[1]: epoch start
2025-01-22 00:55:13,200:[P:2526]:Rank[0/1] => Epoch[1]: train start
2025-01-22 00:55:13,200:[P:2526]:Rank[0/1] => switch to train mode
2025-01-22 00:55:13,229:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:13,229:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:13,229:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:13,252:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:13,252:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:13,253:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:13,469:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:13,470:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:13,470:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:20,649:[P:2526]:Rank[0/1] => Epoch[1][0/13]: Time 7.448s (7.448s)	Speed 8.6 samples/s	Data 0.005s (0.005s)	Loss 6.73969 (6.73969)	Accuracy@1 1.562 (1.562)	Accuracy@5 4.688 (4.688)	
2025-01-22 00:55:20,654:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:20,655:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:20,655:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:20,659:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:20,660:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:20,660:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:20,686:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:20,686:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:20,686:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:27,697:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:27,698:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:27,698:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:27,703:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:27,703:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:27,703:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:27,731:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:27,731:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:27,731:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:34,401:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:34,401:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:34,401:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:34,407:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:34,407:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:34,407:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:34,437:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:34,437:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:34,437:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:41,395:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:41,396:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:41,396:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:41,401:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:41,401:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:41,401:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:41,434:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:41,434:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:41,434:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:47,682:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:47,682:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:47,683:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:47,687:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:47,688:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:47,688:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:47,715:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:47,716:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:47,716:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:55:54,309:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:55:54,309:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:54,309:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:54,314:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:55:54,314:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:54,314:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:55:54,341:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:55:54,341:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:55:54,341:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:01,112:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:56:01,112:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:01,112:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:01,117:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:01,117:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:01,117:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:01,144:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:01,145:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:56:01,145:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:07,577:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:56:07,577:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:07,577:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:07,582:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:07,582:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:07,582:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:07,609:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:07,609:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:56:07,610:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:14,110:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:56:14,110:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:14,111:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:14,115:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:14,115:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:14,115:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:14,142:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:14,143:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:56:14,143:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:20,570:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:56:20,570:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:20,570:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:20,575:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:20,575:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:20,575:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:20,607:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:20,607:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:56:20,607:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:27,016:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:56:27,016:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:27,016:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:27,020:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:27,021:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:27,021:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:27,050:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:27,050:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:56:27,050:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:33,206:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 3, 224, 224])
2025-01-22 00:56:33,206:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:33,206:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:33,211:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 64, 56, 56])
2025-01-22 00:56:33,211:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:33,211:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:33,236:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([64, 192, 28, 28])
2025-01-22 00:56:33,237:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([64, 384, 14, 14])
2025-01-22 00:56:33,237:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([64, 1, 384])
2025-01-22 00:56:39,839:[P:2526]:Rank[0/1] => Epoch[1]: train end, duration: 86.64s
2025-01-22 00:56:39,839:[P:2526]:Rank[0/1] => Epoch[1]: validate start
2025-01-22 00:56:39,840:[P:2526]:Rank[0/1] => switch to eval mode
2025-01-22 00:56:40,117:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 3, 224, 224])
2025-01-22 00:56:40,117:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:56:40,117:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:40,122:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:56:40,122:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:56:40,122:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:40,152:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:56:40,152:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 384, 14, 14])
2025-01-22 00:56:40,152:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([32, 1, 384])
2025-01-22 00:56:41,761:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 3, 224, 224])
2025-01-22 00:56:41,761:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:56:41,761:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:41,764:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 64, 56, 56])
2025-01-22 00:56:41,764:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:56:41,764:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:41,780:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([32, 192, 28, 28])
2025-01-22 00:56:41,780:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([32, 384, 14, 14])
2025-01-22 00:56:41,780:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([32, 1, 384])
2025-01-22 00:56:43,497:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([29, 3, 224, 224])
2025-01-22 00:56:43,497:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([29, 64, 56, 56])
2025-01-22 00:56:43,497:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:43,501:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([29, 64, 56, 56])
2025-01-22 00:56:43,501:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([29, 192, 28, 28])
2025-01-22 00:56:43,501:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] is of type <class 'NoneType'>
2025-01-22 00:56:43,521:[P:2526]:Rank[0/1] Module: VisionTransformer, Input[0] Shape: torch.Size([29, 192, 28, 28])
2025-01-22 00:56:43,521:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[0] Shape: torch.Size([29, 384, 14, 14])
2025-01-22 00:56:43,521:[P:2526]:Rank[0/1] Module: VisionTransformer, Output[1] Shape: torch.Size([29, 1, 384])
2025-01-22 00:56:45,101:[P:2526]:Rank[0/1] => synchronize...
2025-01-22 00:56:45,101:[P:2526]:Rank[0/1] => TEST:	Loss 3.6361	Error@1 74.194%	Error@5 0.000%	Accuracy@1 25.806%	Accuracy@5 100.000%	
2025-01-22 00:56:45,102:[P:2526]:Rank[0/1] => switch to train mode
2025-01-22 00:56:45,103:[P:2526]:Rank[0/1] => Epoch[1]: validate end, duration: 5.26s
2025-01-22 00:56:45,103:[P:2526]:Rank[0/1] => lr: 0.00010059999999999999
2025-01-22 00:56:45,105:[P:2526]:Rank[0/1] => saving checkpoint to OUTPUT/imagenet/cvt-13-224x224
2025-01-22 00:56:45,549:[P:2526]:Rank[0/1] => save model to OUTPUT/imagenet/cvt-13-224x224/model_best.pth
2025-01-22 00:56:45,688:[P:2526]:Rank[0/1] => Epoch[1]: epoch end, duration : 92.49s
2025-01-22 00:56:45,688:[P:2526]:Rank[0/1] => save model to OUTPUT/imagenet/cvt-13-224x224/final_state.pth
2025-01-22 00:56:45,823:[P:2526]:Rank[0/1] => finish training
